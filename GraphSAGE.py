import numpy as np
import torch
import torch.nn.functional as F
from torch_geometric.loader import DataLoader
from torch_geometric.data import Data
from torch_geometric.nn import SAGEConv
from torch_geometric.nn import global_max_pool, global_mean_pool
from sklearn.metrics import roc_auc_score, f1_score


class GraphSAGE(torch.nn.Module):
    """
    Graph Convolutional Network
    """
    def __init__(self, dim_in:int, dim_out:int, dim_h1:int=32, num_layers:int=3, lr:float=0.001, wd:float=5e-4, logger=None):
        """
        Constructor
        """ 
        super().__init__()

        ### Define the model
        # Layer 1: SAGEConv
        self.conv1 = SAGEConv(in_channels=dim_in, out_channels=dim_h1)
        self.convs = torch.nn.ModuleList()
        for i in range(num_layers - 1):
            self.convs.append(SAGEConv(dim_h1, dim_h1))
        # Layer 3: Linear
        self.lin1 = torch.nn.Linear(dim_h1, dim_h1)
        self.lin2 = torch.nn.Linear(dim_h1, dim_out)

        ### Parameters and settings
        self._learning_rate = lr
        self._weight_decay = wd
        self._batch_size = 32
        self._num_classes = dim_out
        # Set device
        self._device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.to(self._device)
        # Set logger
        self._logger = logger

        # print info
        self._print_info()


    def reset_parameters(self):
        self.conv1.reset_parameters()
        for conv in self.convs:
            conv.reset_parameters()
        self.lin1.reset_parameters()
        self.lin2.reset_parameters()


    def forward(self, data):
        """
        Forward pass
        """
        x, edge_index, edge_weight, batch = data.x, data.edge_index, data.edge_weight, data.batch
        
        # Layer 1
        h = self.conv1(x, edge_index) 
        h = F.relu(h)

        # Hidden layers
        for conv in self.convs:
            h = conv(h, edge_index) 
            h = F.relu(h)

        # Pooling
        h = global_mean_pool(h, batch)
        
        # Classification layer
        h = self.lin1(h)
        h = F.relu(h)
        h = F.dropout(h, p=0.5, training=self.training)
        h = self.lin2(h)

        return h 
    
    
    def fit(self, num_epocs:int, train_graphs):
        """
        Train the model
        """
        # optimizer
        optimizer = torch.optim.Adam(self.parameters(), lr=self._learning_rate, weight_decay=self._weight_decay)
        # loss function
        loss_fn = torch.nn.CrossEntropyLoss()

        # Train for N epochs
        for epoch in range(num_epocs):
            self.train()
            train_loader = DataLoader(train_graphs, batch_size=self._batch_size, shuffle=True)
            y_pred, y_true = [], []

            loss_all = 0
            for batch in train_loader:
                batch = batch.to(self._device)
                optimizer.zero_grad()
                output = self(batch)
                label = batch.y
                label = F.one_hot(label, num_classes=self._num_classes)
                label = label.type(torch.FloatTensor)
                label = label.to(self._device)
                loss = loss_fn(output, label)
                loss.backward()
                loss_all += batch.num_graphs * loss.item()
                optimizer.step()

                # Accuracy
                y_pred = y_pred + output.argmax(dim=1).tolist()
                y_true = y_true + batch.y.tolist()
            
            train_loss = loss_all / len(train_graphs)
            train_acc = self._accuracy(y_pred, y_true)
            self._logger.info("Epoch {:>3} : Train loss: {:.4f} | Acc: {:.2f} %".format(epoch + 1, train_loss, train_acc))

    def test(self, test_graphs, y_test):
        """
        Test the model
        """
        test_loader = DataLoader(test_graphs, batch_size=len(test_graphs), shuffle=False)
        
        for batch in test_loader:
            batch = batch.to(self._device)
            test_preds = F.softmax(self(batch), dim=1).detach().numpy()
        
        # Compute One Hot Encoding
        y_test_oh = F.one_hot(torch.from_numpy(y_test), num_classes=self._num_classes)

        # Compute AUC ROC
        auc_roc = roc_auc_score(y_test_oh, test_preds.round(), average='macro', multi_class='ovr')
        self._logger.info("Test AUC: {:.2f}".format(auc_roc))
        # Compute F1 Score
        f1 = f1_score(y_test_oh, test_preds.round(), average='weighted')
        self._logger.info("Test F1 Score: {:.2f}".format(f1))

        # Generate test_preds indices instead of probabilities (one hot encoding)
        test_preds_idx = np.argmax(test_preds, axis=1)

        return test_preds_idx
    
    def _accuracy(self, y_pred, y_true):
        """
        Calculate accuracy
        """
        return np.sum(np.array(y_pred) == np.array(y_true)) / len(y_true) * 100
    
    def _print_info(self):
        """
        Print model info
        """
        self._logger.info("++++++++++++++++++++++++")
        self._logger.info("+++ Model: GraphSAGE +++")
        self._logger.info("++++++++++++++++++++++++")
        self._logger.info("Device: {}".format(self._device))
        self._logger.info("Learning rate: {}".format(self._learning_rate))
        self._logger.info("Weight decay: {}".format(self._weight_decay))
        self._logger.info("Batch size: {}".format(self._batch_size))
        self._logger.info("Model architecture:")
        self._logger.info("Layer 1: GCNConv({}, {})".format(self.conv1.in_channels, self.conv1.out_channels))
        for i, conv in enumerate(self.convs):
            self._logger.info("Layer {}: GCNConv({}, {})".format(i+2, conv.in_channels, conv.out_channels))
        self._logger.info("Linear 1: Linear({}, {})".format(self.lin1.in_features, self.lin1.out_features))
        self._logger.info("Linear 2: Linear({}, {})".format(self.lin2.in_features, self.lin2.out_features))